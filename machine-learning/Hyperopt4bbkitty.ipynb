{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092c90f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as '/s/chopin/a/grad/videep/.local/lib/python3.9/site-packages/psutil/_psutil_linux.cpython-39-x86_64-linux-gnu.so' could not be imported from '5.9.3 instead of 5.9.5'.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "from hyperopt import * #probably do not need to import everything\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import statistics       \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from sklearn.metrics import cohen_kappa_score, make_scorer\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import contractions\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a11b78f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce641751",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as '/s/chopin/a/grad/videep/.local/lib/python3.9/site-packages/psutil/_psutil_linux.cpython-39-x86_64-linux-gnu.so' could not be imported from '5.9.3 instead of 5.9.5'.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efcb767",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hostname\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a4f316",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9b9024",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "hunting_set_file ='/s/babbage/b/nobackup/nblancha/public-datasets/ilideep/natesTask/ThotStuff/Data/FinalThots.xlsx'\n",
    "\n",
    "\n",
    "df_hunting = pd.read_excel(hunting_set_file)\n",
    "df_hunting.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031a3b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df_hunting = df_hunting[~df_hunting['Type'].isin(['DV_recall', 'IAM_recall'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3852110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hunting\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2685f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean(text):\n",
    "# Removes all special characters and numericals leaving the alphabets\n",
    "    text = contractions.fix(text)\n",
    "    #text = re.sub('[^A-Za-z]+', ' ', text)\n",
    "    word_tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    #start_words = ['not', 'no', 'nor', 'against']\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    return ' '.join(filtered_sentence)\n",
    "\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "##The give words to take out\n",
    "giveWords = ['deja-vu',\n",
    "             'deja',\n",
    "             'déjà',\n",
    "             'vu',\n",
    "             'memory' ,\n",
    "             'remembered' ,\n",
    "             'thoughts',\n",
    "             'involuntary' ,\n",
    "             'pop',\n",
    "             'popped',\n",
    "             'pops',\n",
    "             'popping',\n",
    "             'recall',\n",
    "             'recalled',\n",
    "             'remember',\n",
    "             'remembering',\n",
    "             'memories',\n",
    "             'involuntarily',\n",
    "             'sudden',\n",
    "             'thought',\n",
    "             'unexpected',\n",
    "             'felt',\n",
    "             'experience',\n",
    "             'unexpecting',\n",
    "             'suddenly',\n",
    "             'think',\n",
    "             'feeling',\n",
    "             'feel']\n",
    "#giveWords = []\n",
    "listOfSentences = df_hunting['Sentence'] \n",
    "correctSentences = [] \n",
    "for sentence in listOfSentences: \n",
    "  \n",
    "  filtered_words = [word for word in sentence.split() if word.lower()  not in giveWords]\n",
    "  final_string = ' '.join(filtered_words)\n",
    "  correctSentences.append(final_string)\n",
    "  #print(final_string)\n",
    "df_hunting['Sentence'] = correctSentences\n",
    "df_hunting['Sentence'] = df_hunting['Sentence'].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6e4047",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hunting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b80c55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hunting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b167b974",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Features from the dataset\n",
    "#Row start : Row end , Column Start , Column end \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
    "Tfidf_vect.fit(df_hunting['Sentence'])\n",
    "X_Tfidf = Tfidf_vect.transform(df_hunting['Sentence'])\n",
    "#df_hunting['embedding'] = X_Tfidf.toarray()\n",
    "df_hunting = pd.concat([df_hunting,pd.DataFrame(X_Tfidf.todense(), index=df_hunting.index)], axis=1)\n",
    "#pd.DataFrame(X_Tfidf.todense())\n",
    "#df_hunting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6886491b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_hunting.iloc[:, 9:]\n",
    "#X = df_hunting.iloc[  : , 13: ]\n",
    "print(X.columns)\n",
    "X = X.to_numpy()\n",
    "X = X_Tfidf.toarray() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7ac1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hunting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7983b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a126776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Tfidf.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fe5105",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Labels from the dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "y_encoded = encoder.fit_transform(df_hunting['Type'])\n",
    "df_hunting['Type'] = y_encoded\n",
    "\n",
    "y = pd.DataFrame(df_hunting['Type'], columns=['Type'])\n",
    "y = y.to_numpy()\n",
    "y=y.reshape(-1)\n",
    "#y_encoded\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d80895",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5e7d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hunting['Type'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece62721",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hunting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309176cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#some instances need to be grouped\n",
    "#df_hunting = df_hunting.sort_values(\"id\")\n",
    "groups = pd.DataFrame(df_hunting['Subject_id'], columns=['Subject_id'])\n",
    "#print(groups)\n",
    "groups = groups.to_numpy()\n",
    "groups = groups.reshape(-1)\n",
    "\n",
    "#this is cross-validation iterator\n",
    "#cv = GroupShuffleSplit(n_splits=59, test_size=1, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0261be55",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aac049",
   "metadata": {},
   "outputs": [],
   "source": [
    "participants = np.unique(groups)\n",
    "#print(participants)\n",
    "len(participants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9028fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (X.shape, y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f11fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hostname\n",
    "\n",
    "output_csv = \"Language-PredictAge-7-13-2023.csv\"\n",
    "results_path = r\"/s/babbage/b/nobackup/nblancha/public-datasets/ilideep/natesTask/ThotStuff/Important\"\n",
    "modelcsv =os.path.join(results_path, output_csv)\n",
    "if not os.path.exists(modelcsv):\n",
    "        os.mkdir(modelcsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95dc3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# group_kfold = GroupKFold(n_splits=2)\n",
    "logo = LeaveOneGroupOut()\n",
    "cv = logo#.split(X, y, groups=groups) #group_kfold #\n",
    "\n",
    "y_pred = cross_val_predict(AdaBoostClassifier(algorithm = 'SAMME', learning_rate =1.8142857142857143, n_estimators = 74), X, y, groups=groups, n_jobs=20,cv=cv, method='predict')\n",
    "#{'algorithm': 'SAMME', 'learning_rate': 1.8142857142857143, 'n_estimators': 74}\n",
    "#'algorithm': 'SAMME', 'learning_rate': 4.1, 'n_estimators': 74\n",
    "#'algorithm': 'SAMME.R', 'learning_rate': 3.610204081632653, 'n_estimators': 74\n",
    "#print(y)\n",
    "f1_hunt = f1_score(y, y_pred, average=None)[0]\n",
    "f1_non_hunt = f1_score(y, y_pred, average=None)[1]\n",
    "acc_overall = accuracy_score(y, y_pred)\n",
    "acc_balanced = balanced_accuracy_score(y, y_pred)\n",
    "kappa = cohen_kappa_score(y, y_pred)\n",
    "roc = roc_auc_score(y, y_pred)\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "cm_0_0 = cm[0][0]\n",
    "cm_0_1 = cm[0][1]\n",
    "cm_1_0 = cm[1][0]\n",
    "cm_1_1 = cm[1][1]\n",
    "scoring = {\n",
    "    'f1': make_scorer(f1_score, average='weighted'),  # You can adjust 'average' as needed\n",
    "    'kappa': make_scorer(cohen_kappa_score),\n",
    "    'accuracy': make_scorer(accuracy_score)\n",
    "    }\n",
    "scores = cross_validate(AdaBoostClassifier(algorithm = 'SAMME', learning_rate = 1.8142857142857143, n_estimators = 74), X, y,groups=groups, n_jobs=20,cv=cv,scoring=scoring)\n",
    "print(\"Mean F1 Score:\", scores['test_f1'].mean())\n",
    "print(\"Mean Kappa Score:\", scores['test_kappa'].mean())\n",
    "print(\"Mean Accuracy Score:\", scores['test_accuracy'].mean())\n",
    "averageF1 = scores['test_f1'].mean()\n",
    "avgerageKappa = scores['test_kappa'].mean()\n",
    "avgerageAccuracy = scores['test_accuracy'].mean()\n",
    "stdF1 = scores['test_f1'].std()\n",
    "stdKappa = scores['test_kappa'].std()\n",
    "stdAccuracy = scores['test_accuracy'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2581e970",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cefef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17238f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07554a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = AdaBoostClassifier(algorithm = 'SAMME.R', learning_rate = 3.610204081632653, n_estimators = 74)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# get feature importance scores\n",
    "feature_importances = clf.feature_importances_\n",
    "vectorizer = TfidfVectorizer()\n",
    "# map feature importance scores to feature names\n",
    "feature_names = np.array(vectorizer.get_feature_names())\n",
    "sorted_idx = feature_importances.argsort()[::-1]\n",
    "feature_names[sorted_idx]\n",
    "\n",
    "# print top 10 important features\n",
    "print(\"Top 10 important features:\")\n",
    "for feature in feature_names[sorted_idx][:10]:\n",
    "    print(\"- {}\".format(feature))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634f6ff0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "072f3c8c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078623f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "logo = LeaveOneGroupOut()\n",
    "logo.get_n_splits(X, y, groups)\n",
    "cv = logo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0855371",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_PYTHON'] = '/s/babbage/b/nobackup/nblancha/merry/conda/envs/hyperTesting/bin/python'\n",
    "os.environ['PYSPARK_DRIVER'] = '/s/babbage/b/nobackup/nblancha/merry/conda/envs/hyperTesting/bin/python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf370b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv = \"8-26-2023_predictType\"\n",
    "results_path = \"./\"\n",
    "modelcsv =os.path.join(results_path, output_csv)\n",
    "if not os.path.exists(modelcsv):\n",
    "        os.mkdir(modelcsv)\n",
    "else:\n",
    "        print(\"WARNING: DIRECTORY ALREADY EXISTS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e678379",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "avg = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0]\n",
    "results = []\n",
    "group_kfold = GroupKFold(n_splits=len(participants))\n",
    "# group_kfold = GroupKFold(n_splits=2)\n",
    "logo = LeaveOneGroupOut()\n",
    "cv = logo#.split(X, y, groups=groups) #group_kfold #\n",
    "\n",
    "#avg = [1.0,2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]\n",
    "avg = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0]\n",
    "print (X.shape, y.shape)\n",
    "\n",
    "best = 0\n",
    "\n",
    "final_score = { \n",
    "               'model_name':[], \n",
    "               'hyper_paramaters':[],\n",
    "               'f1_0':[],\n",
    "               'acc_overall': [],\n",
    "               'acc_balanced':[], \n",
    "               'kappa':[],\n",
    "               'roc':[],\n",
    "               'roc_w':[], \n",
    "               'cm': [],\n",
    "               'roc_avg':[]}\n",
    "\n",
    "def hyperopt_train_test(params, groups, cv):\n",
    "    global avg\n",
    "    global all_results\n",
    "    global final_score\n",
    "    listOfScores = []\n",
    "    model_name = params['type']\n",
    "    predictioncsv = model_name\n",
    "    del params['type']\n",
    "    for item in params.values():\n",
    "        predictioncsv += '_' + str(item)\n",
    "    predictioncsv += '.csv'\n",
    "    \n",
    "    if model_name == 'naive_bayes':\n",
    "        clf = GaussianNB(**params)\n",
    "    elif model_name == 'AdaBoost':\n",
    "        clf = AdaBoostClassifier(**params)\n",
    "    elif model_name == 'LinSVC':\n",
    "        clf = LinearSVC(**params)      \n",
    "    elif model_name == 'decision_tree':\n",
    "        clf = DecisionTreeClassifier(**params, random_state=1) #(**params, n_jobs=6)\n",
    "    elif model_name == 'randomforest':\n",
    "        clf = RandomForestClassifier(**params, random_state=1) #(**params, n_jobs=6)\n",
    "    elif model_name == 'knn':\n",
    "        clf = KNeighborsClassifier(**params)\n",
    "    elif model_name == 'logistic_regression':\n",
    "        clf = LogisticRegression(**params, random_state=1)\n",
    "    elif model_name == 'SVC':\n",
    "        clf = SVC(**params, probability=True)\n",
    "    elif model_name == 'SVM':\n",
    "        clf = svm.SVC(**params, gamma='auto',probability=True)\n",
    "    else:\n",
    "        return 0\n",
    "    #scores =  cross_validate(clf, X, y,scoring=scoring, groups=groups, cv=cv, n_jobs=20, verbose=200)\n",
    "    y_pred = cross_val_predict(clf, X, y, groups=groups, n_jobs=20,cv=cv, verbose=0, method='predict')\n",
    "    #y_pred = cross_val_predict(clf, X, y, n_jobs=20,cv=cv, verbose=0, method='predict')\n",
    "    y_score = cross_val_predict(clf, X, y, groups=groups, n_jobs=20,cv=cv, verbose=0, method='predict_proba') # Dont this this is ever used\n",
    "    scoring = {\n",
    "    'f1': make_scorer(f1_score, average='weighted'),  # You can adjust 'average' as needed\n",
    "    'kappa': make_scorer(cohen_kappa_score),\n",
    "    'accuracy': make_scorer(accuracy_score)\n",
    "    }\n",
    "    scores = cross_validate(clf, X, y,groups=groups, n_jobs=20,cv=cv,scoring=scoring)\n",
    "    print(\"Mean F1 Score:\", scores['test_f1'].mean())\n",
    "    print(\"Mean Kappa Score:\", scores['test_kappa'].mean())\n",
    "    print(\"Mean Accuracy Score:\", scores['test_accuracy'].mean())\n",
    "    averageF1 = scores['test_f1'].mean()\n",
    "    avgerageKappa = scores['test_kappa'].mean()\n",
    "    avgerageAccuracy = scores['test_accuracy'].mean()\n",
    "    stdF1 = scores['test_f1'].std()\n",
    "    stdKappa = scores['test_kappa'].std()\n",
    "    stdAccuracy = scores['test_accuracy'].std()\n",
    "    listOfScores.append(scores)\n",
    "    f1_0 = f1_score(y, y_pred, labels=[0,1,2], average='weighted')\n",
    "    #print('This is y_pred',y_pred)\n",
    "    acc_overall = accuracy_score(y, y_pred)\n",
    "    \n",
    "    acc_balanced = balanced_accuracy_score(y, y_pred)\n",
    "    kappa = cohen_kappa_score(y, y_pred)\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    ###\n",
    "    roc_auc_ovr = {}\n",
    "    weighted_roc = {}\n",
    "    roc_avg = 0\n",
    "    for class_id in range(2):\n",
    "        print(y_score.shape)\n",
    "        prob_tmp = y_score[:, class_id]\n",
    "        true_max_tmp = [1 if y_tmp == class_id else 0 for y_tmp in y]\n",
    "        tpr_list = [0]\n",
    "        fpr_list = [0]\n",
    "        try:\n",
    "            roc_auc_ovr[class_id] = roc_auc_score(true_max_tmp, prob_tmp)\n",
    "            weighted_roc[class_id] = roc_auc_score(true_max_tmp, prob_tmp, average='weighted')\n",
    "            if(class_id > 0): roc_avg += weighted_roc[class_id]\n",
    "        except:\n",
    "            print(f\"Issue with class {class_id}\")\n",
    "            roc_auc_ovr[class_id] = None\n",
    "    roc = roc_avg/4\n",
    "\n",
    "    avg = [model_name, params, f1_0, acc_overall, acc_balanced, kappa, roc_auc_ovr, weighted_roc, cm, roc_avg, y_pred, predictioncsv,y,averageF1,avgerageAccuracy,\n",
    "           avgerageKappa,stdF1,stdAccuracy,stdKappa]\n",
    "   \n",
    "    return avg[2], avg\n",
    "    \n",
    "#define your search space\n",
    "space = hp.choice('classifier_type', [\n",
    "    {\n",
    "        'type': 'AdaBoost',\n",
    "        'n_estimators': hp.choice('AdaBoost_n_estimators', list(range(30, 100, 11))),\n",
    "        'learning_rate': hp.choice('AdaBoost_learning_rate', list(np.linspace(0.1, 4.1, 50))),\n",
    "        'algorithm': hp.choice('AdaBoost_algorithm', ['SAMME', 'SAMME.R'])\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        'type': 'naive_bayes',\n",
    "       \n",
    "    },\n",
    "    {\n",
    "        'type': 'logistic_regression',\n",
    "        'penalty' : hp.choice('logistic_regression_penalty', ['l2', 'none']),\n",
    "        'solver' : hp.choice('logistic_regression_solver', ['lbfgs', 'newton-cg', 'sag', 'saga'])\n",
    "    },\n",
    "    {\n",
    "        'type': 'SVC',\n",
    "        'C': hp.choice('C', [0.1, 1, 10, 100, 1000]),\n",
    "        'gamma': hp.choice('gamma', [1, 0.1, 0.01, 0.001, 0.0001]),\n",
    "        'kernel': hp.choice('kernal', ['rbf', 'sigmoid']),\n",
    "\n",
    "    },\n",
    "#     {\n",
    "# \n",
    "#    \n",
    "#     {\n",
    "#         'type': 'decision_tree',\n",
    "#         'criterion_dt': hp.choice('criterion', [\"gini\", \"entropy\", \"log_loss\"]),\n",
    "#         'splitter': hp.choice('splitter', [\"best\", \"random\"]),\n",
    "#         'max_features': hp.choice('max_features', [\"auto\", \"sqrt\", \"log2\"]),\n",
    "#\n",
    "#     },\n",
    "  \n",
    "    # {\n",
    "    #    'type': 'SVM',\n",
    "    #    'C': hp.loguniform('C', -5, 2),  # Penalty parameter C\n",
    "    #    'kernel': hp.choice('kernel', ['linear', 'poly', 'rbf', 'sigmoid']),  # Kernel type\n",
    "    #    'degree': hp.choice('degree', np.arange(1, 13+1, dtype=int)),  # Degree of the polynomial kernel\n",
    "    #   # 'gamma': hp.loguniform('gamma', -5, 2)  # Kernel coefficient for 'rbf', 'poly', and 'sigmoid'\n",
    "       \n",
    "    # },\n",
    "   {\n",
    "       'type': 'randomforest',\n",
    "       'n_estimators': hp.choice('n_estimators', list(range(20, 170, 16))),\n",
    "       'criterion': hp.choice('criterion', [\"gini\", \"entropy\"]),\n",
    "       'max_features': hp.choice('max_features', [None, \"sqrt\", \"log2\"]),\n",
    "       \n",
    "   },\n",
    "    {\n",
    "        'type': 'knn',\n",
    "        'n_neighbors': hp.choice('knn_n_neighbors', list(range(1, 300, 10)))\n",
    "    }\n",
    "    \n",
    "])\n",
    "count = 1\n",
    "all_results =[]\n",
    "\n",
    "trials = Trials()\n",
    "def f(params):\n",
    "    global best, count, avg, all_results, final_score\n",
    "    count += 1\n",
    "    guide_metric, scores = hyperopt_train_test(params.copy(), groups, cv)\n",
    "    \n",
    "    results.append(scores)\n",
    "    if guide_metric > best:\n",
    "        best = guide_metric\n",
    "    else: \n",
    "        print (scores)\n",
    "    print(avg[13])\n",
    "        \n",
    "    #print ('iters:', count, ', kappa:', guide_metric, 'using', params)\n",
    "    return {'loss': -guide_metric, 'status': STATUS_OK, 'eval_time': time.time(), \n",
    "            'other_stuff': {'model':avg[0],'params':avg[1],'f1_0':avg[2] ,'acc_overall': avg[3],\n",
    "                           'acc_balanced': avg[4],'kappa':avg[5] , 'roc': avg[6],'roc_w': avg[7], 'cm': avg[8], \n",
    "                           'roc_avg': avg[9], 'y_pred': avg[10], 'predcsv' : avg[11],'y_actual':avg[12],'averageF1':avg[13],'avgerageAccuracy':avg[14], \n",
    "                            'avgerageKappa':avg[15], 'stdF1':avg[16], 'stdAccuracy':avg[17],'stdKappa':avg[18] }}\n",
    "# avg = [model_name, params, f1_0, acc_overall, acc_balanced, kappa, roc_auc_ovr, weighted_roc, cm, roc_avg, y_pred, predictioncsv,y,averageF1,avgerageAccuracy,\n",
    "           #avgerageKappa,stdF1,stdAccuracy,stdKappa]\n",
    "#spark = SparkSession.builder.master(\"spark://luffy:30122\").getOrCreate()...,\n",
    "#spark_trials = SparkTrials(parallelism=8, spark_session=spark) #DO NOT USE\n",
    "spark_trials = SparkTrials(parallelism=128)\n",
    "\n",
    "best = fmin(\n",
    "            f, \n",
    "            space, \n",
    "            trials=spark_trials,    \n",
    "            algo=tpe.suggest, \n",
    "            max_evals= 2000,)\n",
    "print ('best:' )\n",
    "print(best)\n",
    "#performance_csv = os.path.join(\"models\", \"performance.csv\")\n",
    "#pd.DataFrame(final_score).to_csv(performance_csv, index=False)\n",
    "\n",
    "# -> {'a': 1, 'c2': 0.01420615366247227}\n",
    "#print(space_eval(space, best))\n",
    "# -> ('case 2', 0.01420615366247227)\n",
    "#df = pd.DataFrame(results, columns=[\"model_name\", \"params\", \"f1_0\", \"acc_overall\", \"acc_balanced\", \"kappa\", \"roc_auc_ovr\", \"weighted_roc\", \"cm\", \"roc_avg\", 'y_pred', 'predcsv'])\n",
    "#df = df.sort_values(by=\"f1_0\", ascending=False)\n",
    "#df.to_csv(modelcsv)\n",
    "#df.to_csv(modelcsv + \".csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aa9ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1639298",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "# Assuming you have your data organized by groups\n",
    "groups = groups # List of group IDs for each sample\n",
    "\n",
    "# Define your classifier 'clf'\n",
    "clf = clf = RandomForestClassifier(n_estimators=64, criterion='gini',max_depth=56) # Instantiate your classifier here\n",
    "\n",
    "# Perform cross-validated predictions with GroupKFold\n",
    "cv = GroupKFold(n_splits=314)\n",
    "fold_predictions_list = []\n",
    "\n",
    "for fold, (train_indices, test_indices) in enumerate(cv.split(X, y, groups)):\n",
    "    X_train, y_train = X[train_indices], y[train_indices]\n",
    "    X_test = X[test_indices]\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    fold_predictions = clf.predict(X_test)\n",
    "    fold_predictions_list.append(fold_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f524cb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698519fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "\n",
    "# Configure Spark to connect to the existing cluster\n",
    "conf = SparkConf().setMaster(\"spark://luffy:30122\")\n",
    "\n",
    "# Create SparkSession with the configured options\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b8ef31",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fae5d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(spark_trials.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc45bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_trials.trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393d9e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_results = []\n",
    "for trial in spark_trials.trials:\n",
    "    best_results.append(trial['result']['other_stuff'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a23097",
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfAllModels = []\n",
    "for eachModel in best_results:\n",
    "    df = pd.DataFrame([eachModel])\n",
    "    print(df)\n",
    "    listOfAllModels.append(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b6d580",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df = pd.concat(listOfAllModels, ignore_index=True)\n",
    "#df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e848fbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a2b917",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df.to_csv('3-13-24-PredictTypeUsingLanguage.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348ca2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = concatenated_df['y_pred']\n",
    "y_actual = concatenated_df['y_actual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dc71fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(y_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77395dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2955d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohen_kappa_score(np.array(y_actual[0]),np.array(y_pred[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f41fed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44366b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.17"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
